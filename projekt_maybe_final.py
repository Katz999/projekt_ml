# -*- coding: utf-8 -*-
"""projekt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AfgniYkKznbOzg6pjYsuAA5HRiVeQ8aN
"""

# Importing necessary libraries
import os
import numpy as np
import pandas as pd
import warnings
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn import preprocessing
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import matplotlib
matplotlib.style.use('ggplot')
warnings.filterwarnings("ignore")

# Problem Definition
# The goal is to predict the presence of heart disease in patients using various machine learning models.
# We will evaluate the performance of these models using appropriate metrics and optimize their hyperparameters.

# Load the data
df = pd.read_csv('./heart.csv')

# Data Inspection and Preprocessing
# Convert categorical columns to string types
string_col = df.select_dtypes(include="object").columns
df[string_col]=df[string_col].astype("string")

# Separate categorical and numerical columns
string_col = df.select_dtypes("string").columns.to_list()
num_col = df.columns.to_list()
for col in string_col:
    num_col.remove(col)
num_col.remove("HeartDisease")

# Check for missing values
print("Missing values:\n", df.isnull().sum())  # No missing values found

# Data Visualization
fig = px.histogram(df, x="HeartDisease", color="Sex", hover_data=df.columns, title="Distribution of Heart Diseases", barmode="group")
fig.show()

fig = px.histogram(df, x="ChestPainType", color="Sex", hover_data=df.columns, title="Types of Chest Pain")
fig.show()

fig = px.histogram(df, x="Sex", hover_data=df.columns, title="Sex Ratio in the Data")
fig.show()

fig = px.histogram(df, x="RestingECG", hover_data=df.columns, title="Distribution of Resting ECG")
fig.show()

plt.figure(figsize=(15,10))
sns.pairplot(df, hue="HeartDisease")
plt.title("Looking for Insights in Data")
plt.legend("HeartDisease")
plt.tight_layout()
plt.show()

plt.figure(figsize=(15,10))
for i, col in enumerate(df.columns, 1):
    plt.subplot(4, 3, i)
    plt.title(f"Distribution of {col} Data")
    sns.histplot(df[col], kde=True)
    plt.tight_layout()
plt.show()

fig = px.box(df, y="Age", x="HeartDisease", title="Distribution of Age")
fig.show()

fig = px.box(df, y="RestingBP", x="HeartDisease", title="Distribution of RestingBP", color="Sex")
fig.show()

fig = px.box(df, y="Cholesterol", x="HeartDisease", title="Distribution of Cholesterol")
fig.show()

fig = px.box(df, y="Oldpeak", x="HeartDisease", title="Distribution of Oldpeak")
fig.show()

fig = px.box(df, y="MaxHR", x="HeartDisease", title="Distribution of MaxHR")
fig.show()

# Transforming categorical data using one-hot encoding
df = pd.get_dummies(df, columns=string_col, drop_first=True)

# Scaling numerical data
scaler = StandardScaler()
df[num_col] = scaler.fit_transform(df[num_col])

# Splitting data into training and test sets
X = df.drop(columns=["HeartDisease"])
y = df["HeartDisease"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Model Selection and Training
models = {
    "Logistic Regression": LogisticRegression(),
    "Naive Bayes": GaussianNB(),
    "SVM (Linear Kernel)": SVC(kernel="linear"),
    "SVM (RBF Kernel)": SVC(kernel="rbf"),
    "KNN": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "XGBoost": XGBClassifier()
}

# Hyperparameter tuning using GridSearchCV
param_grids = {
    "Logistic Regression": {"C": [0.1, 1, 10, 100]},
    "SVM (Linear Kernel)": {"C": [0.1, 1, 10, 100]},
    "SVM (RBF Kernel)": {"C": [0.1, 1, 10, 100], "gamma": [0.001, 0.01, 0.1, 1]},
    "KNN": {"n_neighbors": [3, 5, 11, 19], "metric": ["euclidean", "manhattan"]},
    "Decision Tree": {"max_depth": [None, 10, 20, 30, 40, 50], "min_samples_split": [2, 5, 10]},
    "Random Forest": {"n_estimators": [10, 50, 100, 200], "criterion": ["gini", "entropy"]},
    "XGBoost": {"learning_rate": [0.01, 0.1, 0.2, 0.3], "max_depth": [3, 5, 7, 9], "n_estimators": [50, 100, 200]}
}

best_models = {}
for model_name, model in models.items():
    param_grid = param_grids.get(model_name, {})
    grid_search = GridSearchCV(model, param_grid, cv=StratifiedKFold(n_splits=5), scoring='roc_auc', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_models[model_name] = grid_search.best_estimator_
    print(f"Best parameters for {model_name}: {grid_search.best_params_}")

# Evaluating the models
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred)
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1 Score: {f1}")
    print(f"ROC AUC: {roc_auc}")
    print(classification_report(y_test, y_pred))
    return accuracy, precision, recall, f1, roc_auc

results = {}
for model_name, model in best_models.items():
    print(f"\nEvaluating {model_name}")
    results[model_name] = evaluate_model(model, X_test, y_test)

# Visualizing Feature Importance for RandomForest and XGBoost
def plot_feature_importance(model, X_train):
    feature_importance = model.feature_importances_
    sorted_idx = np.argsort(feature_importance)
    plt.figure(figsize=(10, 10))
    plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align="center")
    plt.yticks(range(len(sorted_idx)), [X_train.columns[i] for i in sorted_idx])
    plt.xlabel("Feature Importance")
    plt.title("Feature Importance in the Model")
    plt.show()

plot_feature_importance(best_models["Random Forest"], X_train)
plot_feature_importance(best_models["XGBoost"], X_train)

# Ensuring the model is well-fitted
# Check if the model is overfitted or underfitted by comparing training and validation performance
def check_overfitting(model, X_train, y_train, X_test, y_test):
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    train_accuracy = accuracy_score(y_train, y_train_pred)
    test_accuracy = accuracy_score(y_test, y_test_pred)
    print(f"Train Accuracy: {train_accuracy}")
    print(f"Test Accuracy: {test_accuracy}")
    if train_accuracy > test_accuracy + 0.05:
        print("Model might be overfitted.")
    elif test_accuracy > train_accuracy + 0.05:
        print("Model might be underfitted.")
    else:
        print("Model is well-fitted.")

for model_name, model in best_models.items():
    print(f"\nChecking overfitting for {model_name}")
    check_overfitting(model, X_train, y_train, X_test, y_test)




